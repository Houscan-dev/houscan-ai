import os
import torch
import json
import datetime
import re
import chromadb
from chromadb.config import Settings
# LLM ì‚¬ìš©ì„ ìœ„í•œ transformers ì¶”ê°€
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModel # ë˜ëŠ” AutoModelForSeq2SeqLM
import time # LLM ì‘ë‹µ ì‹œê°„ ì¸¡ì •ì„ ìœ„í•´ ì¶”ê°€ (ì„ íƒ ì‚¬í•­)

# --- ê¸°ì¡´ ChromaDB ë° ì„ë² ë”© ì„¤ì • (ìœ ì§€) ---
# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •)
embedding_tokenizer = AutoTokenizer.from_pretrained("BAAI/bge-m3")
embedding_model = AutoModel.from_pretrained("BAAI/bge-m3")
embedding_model.eval()

# ChromaDB í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
chroma_client = chromadb.PersistentClient(path="./chroma_db")
collection = chroma_client.get_or_create_collection(name="processed_chunks")

def get_embedding(text):
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (ê¸°ì¡´ ì½”ë“œ í™œìš©)"""
    inputs = embedding_tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding=True)
    with torch.no_grad():
        outputs = embedding_model(**inputs)
        embedding = outputs.last_hidden_state[:, 0, :]
    return embedding.squeeze().tolist()

# --- LLM ë¡œë”© ë° ì„¤ì • ---
# í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ì‚¬ìš©í•  LLM ëª¨ë¸ ì„ íƒ (ì˜ˆì‹œ, í•„ìš”ì‹œ ë³€ê²½)
# í•œêµ­ì–´ ì§€ì› ë° Instruction Following ëŠ¥ë ¥ì´ ì¢‹ì€ ëª¨ë¸ ì„ íƒ í•„ìš”
# ì˜ˆì‹œ 1: KoAlpaca (Polyglot ê¸°ë°˜) - ë¹„êµì  ê°€ë²¼ì›€
# llm_model_name = "beomi/KoAlpaca-Polyglot-12.8B"
# ì˜ˆì‹œ 2: Smaller T5 variant (ì‹¤í—˜ í•„ìš”)
# llm_model_name = "google/flan-t5-large" # í•œêµ­ì–´ ì„±ëŠ¥ ë° JSON ì¶œë ¥ ëŠ¥ë ¥ í™•ì¸ í•„ìš”
# ì˜ˆì‹œ 3: Larger model (ë¦¬ì†ŒìŠ¤ ìš”êµ¬ëŸ‰ ë†’ìŒ)
# llm_model_name = "HuggingFaceH4/zephyr-7b-beta" # í•œêµ­ì–´ ì„±ëŠ¥ ë° JSON ì¶œë ¥ ëŠ¥ë ¥ í™•ì¸ í•„ìš”

# ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë¡œ êµì²´í•´ì•¼ í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œ ì´ë¦„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
# ë¦¬ì†ŒìŠ¤ ì œì•½ì´ ìˆë‹¤ë©´ ë” ì‘ì€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê±°ë‚˜ API í˜•íƒœì˜ LLM ì‚¬ìš©ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.
llm_model_name = "Upstage/SOLAR-10.7B-Instruct-v1.0" # <--- !!! ì¤‘ìš”: ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ LLM ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ë°˜ë“œì‹œ ë³€ê²½í•˜ì„¸ìš” !!!
                        # gpt2ëŠ” ì˜ˆì‹œì¼ ë¿ì´ë©°, í•œêµ­ì–´/Instruction/JSON ì¶œë ¥ì— ì í•©í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                        # KoAlpaca, Solar, Llama-ko ë“± ê³ ë ¤

print(f"Loading LLM model: {llm_model_name}...")
try:
    # ëª¨ë¸ ë¡œë”© ë°©ì‹ì€ ëª¨ë¸ íƒ€ì…(CausalLM, Seq2SeqLM)ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
    llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name)
    # CausalLM ì˜ˆì‹œ (GPT, Llama ë“±)
    llm_model = AutoModelForCausalLM.from_pretrained(llm_model_name)
    # Seq2SeqLM ì˜ˆì‹œ (T5, BART ë“±)
    # llm_model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)

    # GPU ì‚¬ìš© ì„¤ì • (ê°€ëŠ¥í•˜ë‹¤ë©´)
    if torch.cuda.is_available():
        llm_model.to("cuda")
    llm_model.eval() # ì¶”ë¡  ëª¨ë“œ ì„¤ì •

    # íŒŒì´í”„ë¼ì¸ ì‚¬ìš© (ë” ê°„í¸í•  ìˆ˜ ìˆìŒ)
    # text_generator = pipeline("text-generation", model=llm_model, tokenizer=llm_tokenizer, device=0 if torch.cuda.is_available() else -1)
    print("LLM model loaded successfully.")
except Exception as e:
    print(f"Error loading LLM model: {e}")
    print("Please ensure the model name is correct and you have enough resources.")
    # LLM ë¡œë”© ì‹¤íŒ¨ ì‹œ ì¢…ë£Œ ë˜ëŠ” ë‹¤ë¥¸ ì²˜ë¦¬
    exit()

# --- ìê²© ì¡°ê±´ ì¶”ì¶œ ë° ê²€ì¦ í•¨ìˆ˜ ---

def get_relevant_chunks(pdf_name, relevant_titles):
    """ íŠ¹ì • PDFì—ì„œ ê´€ë ¨ ì œëª©ì„ ê°€ì§„ ì²­í¬ë“¤ì„ ê°€ì ¸ì˜´ """
    where_conditions = {
        "filename": pdf_name,
        "$or": [{"title": title} for title in relevant_titles]
    }
    results = collection.get(where=where_conditions)

    if not results or not results['documents']:
        print(f"Warning: No chunks found for titles {relevant_titles} in {pdf_name}")
        return ""

    # ì²­í¬ ì¸ë±ìŠ¤ ìˆœì„œëŒ€ë¡œ ì •ë ¬ (ì„ íƒ ì‚¬í•­, ë¬¸ë§¥ ì—°ê²°ì— ë„ì›€ë  ìˆ˜ ìˆìŒ)
    chunks_with_meta = list(zip(results['documents'], results['metadatas']))
    chunks_with_meta.sort(key=lambda x: x[1].get('chunk_index', 0))

    # ê´€ë ¨ ì²­í¬ ë‚´ìš© í•©ì¹˜ê¸°
    # HTML íƒœê·¸ ì œê±° (LLM ì…ë ¥ ì „ì²˜ë¦¬)
    combined_text = ""
    for doc, meta in chunks_with_meta:
        # ì œëª© íƒœê·¸ ì œê±° ë° ê¸°ë³¸ ì „ì²˜ë¦¬
        content_cleaned = re.sub(r'<h\d>.*?</h\d>\n?', '', doc, 1)
        content_cleaned = content_cleaned.strip()
        combined_text += content_cleaned + "\n\n" # ì²­í¬ ê°„ êµ¬ë¶„

    return combined_text

def generate_llm_response(context, max_new_tokens=512):
    """ LLMì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„± (JSON í˜•ì‹ ì¶”ì¶œ ìœ ë„) """

    # --- í”„ë¡¬í”„íŠ¸ ì •ì˜ ---
    prompt = f"""ë‹¤ìŒì€ ì£¼íƒ ì…ì£¼ì ëª¨ì§‘ ê³µê³ ë¬¸ì˜ ì¼ë¶€ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì²­ë…„ ì‹ ì²­ìì˜ ì£¼ìš” ì…ì£¼ ìê²© ìš”ê±´ì„ ì¶”ì¶œí•˜ì—¬ **ë°˜ë“œì‹œ JSON í˜•ì‹**ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ê° í•­ëª©ì˜ ê°’ì´ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ë˜ì§€ ì•Šì•˜ë‹¤ë©´ nullì„ ì‚¬ìš©í•˜ì„¸ìš”. JSON ì™¸ì˜ ì„¤ëª…ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

ìš”êµ¬ í•­ëª©:
- "age_min": ìµœì†Œ ë‚˜ì´ (ë§Œ ë‚˜ì´ ê¸°ì¤€, ì •ìˆ˜)
- "age_max": ìµœëŒ€ ë‚˜ì´ (ë§Œ ë‚˜ì´ ê¸°ì¤€, ì •ìˆ˜)
- "must_be_unmarried": í˜¼ì¸ ìƒíƒœ (ë¯¸í˜¼ì´ì–´ì•¼ í•˜ëŠ”ì§€ ì—¬ë¶€, ë¶ˆë¦¬ì–¸)
- "income_limit_percent": ì†Œë“ ê¸°ì¤€ (ì „ë…„ë„ ë„ì‹œê·¼ë¡œì ì›”í‰ê· ì†Œë“ ëŒ€ë¹„ ë¹„ìœ¨, ì •ìˆ˜, ì˜ˆ: 100)
- "total_asset_limit_won": ì´ìì‚° ê¸°ì¤€ (ì„¸ëŒ€ ê¸°ì¤€, ì› ë‹¨ìœ„, ì •ìˆ˜)
- "car_asset_limit_won": ìë™ì°¨ ê¸°ì¤€ (ì„¸ëŒ€ ê¸°ì¤€, ì› ë‹¨ìœ„, ì •ìˆ˜)
- "must_be_homeless": ë¬´ì£¼íƒ ìš”ê±´ (ì„¸ëŒ€ ê¸°ì¤€, ë¶ˆë¦¬ì–¸)

[ê³µê³ ë¬¸ ë‚´ìš© ì‹œì‘]
{context}
[ê³µê³ ë¬¸ ë‚´ìš© ë]

JSON ì‘ë‹µ:
"""

    print("\n--- Sending Prompt to LLM ---")
    # print(prompt) # í”„ë¡¬í”„íŠ¸ ë‚´ìš© í™•ì¸ (ë””ë²„ê¹… ì‹œ)
    print("--- End of Prompt ---")

    start_time = time.time()
    try:
        # ë°©ë²• 1: pipeline ì‚¬ìš© ì‹œ
        # generated = text_generator(prompt, max_length=len(prompt.split()) + max_new_tokens, num_return_sequences=1, pad_token_id=llm_tokenizer.eos_token_id)
        # response_text = generated[0]['generated_text'][len(prompt):] # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œì™¸

        # ë°©ë²• 2: ëª¨ë¸ ì§ì ‘ ì‚¬ìš© ì‹œ (CausalLM ì˜ˆì‹œ)
        inputs = llm_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048) # ëª¨ë¸ì˜ ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë§ì¶° ì¡°ì •
        if torch.cuda.is_available():
            inputs = inputs.to("cuda")

        # í† í° ìƒì„± ì˜µì…˜ ì„¤ì •
        # temperature, top_p ë“± ì¡°ì ˆí•˜ì—¬ ê²°ê³¼ í’ˆì§ˆ ê°œì„  ì‹œë„ ê°€ëŠ¥
        outputs = llm_model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            no_repeat_ngram_size=3, # ë°˜ë³µ ì¤„ì´ê¸°
            # early_stopping=True, # í•„ìš”ì‹œ
            pad_token_id=llm_tokenizer.eos_token_id,
            # temperature=0.7, # ì•½ê°„ì˜ ì°½ì˜ì„± í—ˆìš©
            # top_p=0.9,
            # do_sample=True # ìƒ˜í”Œë§ ì‚¬ìš© ì—¬ë¶€
        )
        response_text = llm_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)

        end_time = time.time()
        print(f"LLM response generated in {end_time - start_time:.2f} seconds.")
        print("\n--- LLM Raw Response ---")
        print(response_text)
        print("--- End of LLM Raw Response ---")
        return response_text

    except Exception as e:
        print(f"Error during LLM generation: {e}")
        return None

def parse_llm_json_output(llm_output):
    """ LLM ì¶œë ¥ì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ê³  íŒŒì‹± """
    if not llm_output:
        return None

    # LLM ì‘ë‹µì—ì„œ JSON ê°ì²´ë§Œ ì •í™•íˆ ì¶”ì¶œ ì‹œë„
    # ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•: ```json ... ``` ë¸”ë¡ ì°¾ê¸°
    match_code_block = re.search(r'```json\s*(\{.*?\})\s*```', llm_output, re.DOTALL | re.IGNORECASE)
    # ë˜ëŠ” ì‘ë‹µ ì‹œì‘ì´ { ë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸
    match_direct_json = re.search(r'^\s*(\{.*?\})\s*$', llm_output, re.DOTALL)

    json_str = None
    if match_code_block:
        json_str = match_code_block.group(1)
    elif match_direct_json:
         json_str = match_direct_json.group(1)
    else:
        # LLMì´ JSONë§Œ ë°˜í™˜í•˜ë„ë¡ ìœ ë„í–ˆìœ¼ë¯€ë¡œ, ì „ì²´ ì‘ë‹µì´ JSONì¼ ìˆ˜ ìˆìŒ
        # ë˜ëŠ”, JSONì´ ê¹¨ì¡Œê±°ë‚˜ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ì™€ ì„ì˜€ì„ ìˆ˜ ìˆìŒ
        # ì‹œì‘ '{' ì™€ ë§ˆì§€ë§‰ '}'ë¥¼ ì°¾ì•„ ì¶”ì¶œ ì‹œë„ (ê°€ì¥ ë¶ˆì•ˆì •)
        start_index = llm_output.find('{')
        end_index = llm_output.rfind('}')
        if start_index != -1 and end_index != -1 and start_index < end_index:
            json_str = llm_output[start_index : end_index + 1]
        else:
            print("Warning: Could not find JSON structure in LLM output.")
            return None

    try:
        parsed_json = json.loads(json_str)
        print("\n--- Parsed JSON from LLM ---")
        print(json.dumps(parsed_json, indent=2, ensure_ascii=False))
        print("--- End of Parsed JSON ---")
        return parsed_json
    except json.JSONDecodeError as e:
        print(f"Error parsing JSON from LLM output: {e}")
        print("LLM Output causing error:\n", json_str)
        return None

# ê¸ˆì•¡ ë‹¨ìœ„ ë³€í™˜ í•¨ìˆ˜ (ê¸°ì¡´ ì½”ë“œ í™œìš©)
def convert_to_won(value_str, unit_indicator):
    """ ê¸ˆì•¡ ë‹¨ìœ„ ë³€í™˜ (LLM ì¶”ì¶œê°’ì€ ì´ë¯¸ ìˆ«ìì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•½ê°„ ìˆ˜ì •) """
    try:
        # LLMì´ ì´ë¯¸ ìˆ«ìë¡œ ì¶”ì¶œí–ˆì„ ê²½ìš°
        if isinstance(value_str, (int, float)):
             return int(value_str)
        # LLMì´ ë¬¸ìì—´ë¡œ ì¶”ì¶œí–ˆì„ ê²½ìš° (ì˜ˆ: "3ì–µ 803ë§Œì›") - ì¶”ê°€ ì²˜ë¦¬ í•„ìš”
        # ê°„ë‹¨íˆ ìˆ«ìë§Œ ì¶”ì¶œ ì‹œë„
        value_str = re.sub(r'[^\d]', '', value_str)
        value = int(value_str)

        # ì›ë³¸ í…ìŠ¤íŠ¸ì— 'ì–µì›'ì´ë‚˜ 'ë§Œì›'ì´ ìˆì—ˆëŠ”ì§€ í™•ì¸ í•„ìš” (í˜„ì¬ êµ¬ì¡°ë¡œëŠ” ì–´ë ¤ì›€)
        # ì—¬ê¸°ì„œëŠ” LLMì´ ì› ë‹¨ìœ„ë¡œ ì˜ ì¶”ì¶œí–ˆë‹¤ê³  ê°€ì •í•˜ê±°ë‚˜,
        # í”„ë¡¬í”„íŠ¸ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ì› ë‹¨ìœ„ë¡œ ë°˜í™˜í•˜ë„ë¡ ìš”êµ¬í•´ì•¼ í•¨.
        # ë§Œì•½ LLMì´ "33700ë§Œì›" ì²˜ëŸ¼ ë°˜í™˜í•˜ë©´ í›„ì²˜ë¦¬ í•„ìš”.
        # -> ê°€ì¥ ì¢‹ì€ ë°©ë²•ì€ í”„ë¡¬í”„íŠ¸ì—ì„œ 'ì› ë‹¨ìœ„ ì •ìˆ˜'ë¡œ ëª…ì‹œí•˜ëŠ” ê²ƒ.
        return value
    except Exception as e:
        print(f"Warning: Could not convert value '{value_str}' to Won: {e}")
        return None


def check_eligibility_with_llm(pdf_name, user_info):
    """ LLMì„ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ê³µê³ ë¬¸ì˜ ìê²© ìš”ê±´ ê²€ì¦ """

    # 1. ê´€ë ¨ ì²­í¬ ê²€ìƒ‰ (ì œëª© ê¸°ë°˜)
    # ì´ ê³µê³ ë¬¸ì˜ ì‹¤ì œ ì œëª© íƒœê·¸ë¥¼ í™•ì¸í•˜ì—¬ ìˆ˜ì • í•„ìš”
    relevant_titles = [
        "1.  ê³µí†µì‚¬í•­",  # chunk_index: 4
        "â–¡ ì…ì£¼ìëª¨ì§‘ê³µê³ ì¼(2025.03.07) í˜„ì¬ ë¬´ì£¼íƒì„¸ëŒ€êµ¬ì„±ì›ìœ¼ë¡œì„œ ì•„ë˜ì˜ ìš”ê±´ì„ ëª¨ë‘ ê°–ì¶˜ ì", # chunk_index: 8
        "â–¡ ì†Œë“ ë° ìì‚° ê¸°ì¤€" # chunk_index: 9, 10, 11
        # í•„ìš”ì‹œ ë‹¤ë¥¸ ê´€ë ¨ ì œëª© ì¶”ê°€
    ]
    context_text = get_relevant_chunks(pdf_name, relevant_titles)

    if not context_text:
        print(f"Error: Could not retrieve relevant context for {pdf_name}")
        return {"pdf_name": pdf_name, "eligible": None, "reason": "Relevant context not found"}

    # 2. LLM í˜¸ì¶œí•˜ì—¬ ìê²© ì¡°ê±´ ì¶”ì¶œ
    llm_output = generate_llm_response(context_text)
    extracted_criteria = parse_llm_json_output(llm_output)

    if not extracted_criteria:
        print(f"Error: Failed to extract criteria using LLM for {pdf_name}")
        return {"pdf_name": pdf_name, "eligible": None, "reason": "Failed to extract criteria using LLM"}

    # 3. ìê²© ì¡°ê±´ ë¹„êµ ë¶„ì„
    analysis_result = {
        "pdf_name": pdf_name,
        "eligible": True, # ê¸°ë³¸ê°’ì„ Trueë¡œ ì„¤ì •í•˜ê³ , ë¯¸ì¶©ì¡± ì‹œ Falseë¡œ ë³€ê²½
        "matched_conditions": [],
        "unmet_conditions": [],
        "extracted_criteria": extracted_criteria # LLMì´ ì¶”ì¶œí•œ ì›ë³¸ ì •ë³´ í¬í•¨
    }

    # ë‚˜ì´ ê²€ì¦
    birth_year = int(user_info["birth_date"][:4])
    current_year = datetime.datetime.now().year
    # ë§Œ ë‚˜ì´ ê³„ì‚° (í•œêµ­ì‹ ë‚˜ì´ ëŒ€ì‹ )
    # ìƒì¼ì´ ì§€ë‚¬ëŠ”ì§€ ì—¬ë¶€ê¹Œì§€ ê³ ë ¤í•˜ë©´ ë” ì •í™•í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ì—°ë„ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
    age_international = current_year - birth_year
    # ë§Œì•½ ê³µê³ ì¼ ê¸°ì¤€ ë§Œ ë‚˜ì´ê°€ í•„ìš”í•˜ë‹¤ë©´ ê³µê³ ì¼ ì‚¬ìš© (ì—¬ê¸°ì„œëŠ” í˜„ì¬ ì—°ë„ ê¸°ì¤€)
    age_criterion_met = True
    if extracted_criteria.get("age_min") is not None and age_international < extracted_criteria["age_min"]:
        age_criterion_met = False
    if extracted_criteria.get("age_max") is not None and age_international > extracted_criteria["age_max"]:
        age_criterion_met = False

    if age_criterion_met:
         if extracted_criteria.get("age_min") is not None or extracted_criteria.get("age_max") is not None:
              age_req_str = f"ë§Œ {extracted_criteria.get('age_min', '?')}ì„¸ ~ {extracted_criteria.get('age_max', '?')}ì„¸"
              analysis_result["matched_conditions"].append(f"ì—°ë ¹ ì¡°ê±´ ì¶©ì¡±: {age_req_str} (í˜„ì¬ ë§Œ {age_international}ì„¸)")
    else:
        age_req_str = f"ë§Œ {extracted_criteria.get('age_min', '?')}ì„¸ ~ {extracted_criteria.get('age_max', '?')}ì„¸"
        analysis_result["eligible"] = False
        analysis_result["unmet_conditions"].append(f"ì—°ë ¹ ì¡°ê±´ ë¯¸ì¶©ì¡±: ìš”êµ¬ì‚¬í•­={age_req_str}, ì‚¬ìš©ì=ë§Œ {age_international}ì„¸")

    # í˜¼ì¸ ìƒíƒœ ê²€ì¦
    must_be_unmarried = extracted_criteria.get("must_be_unmarried")
    if must_be_unmarried is True: # LLMì´ booleanìœ¼ë¡œ ë°˜í™˜í–ˆë‹¤ê³  ê°€ì •
        # user_infoì— í˜¼ì¸ ìƒíƒœ í•„ë“œê°€ í•„ìš”í•¨. ì—†ìœ¼ë¯€ë¡œ ì„ì‹œë¡œ 'ë¯¸í˜¼'ìœ¼ë¡œ ê°€ì •.
        user_is_married = False # ì˜ˆì‹œ: ì‹¤ì œ user_infoì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨
        if user_is_married:
            analysis_result["eligible"] = False
            analysis_result["unmet_conditions"].append("í˜¼ì¸ ì¡°ê±´ ë¯¸ì¶©ì¡±: ë¯¸í˜¼ì´ì–´ì•¼ í•¨")
        else:
            analysis_result["matched_conditions"].append("í˜¼ì¸ ì¡°ê±´ ì¶©ì¡±: ë¯¸í˜¼")
    elif must_be_unmarried is False:
         analysis_result["matched_conditions"].append("í˜¼ì¸ ì¡°ê±´ ë¬´ê´€")
    # nullì¸ ê²½ìš°ëŠ” ì¡°ê±´ì´ ì—†ê±°ë‚˜ ì¶”ì¶œ ì‹¤íŒ¨ë¡œ ê°„ì£¼

    # ì†Œë“ ê²€ì¦
    income_limit_percent = extracted_criteria.get("income_limit_percent")
    if income_limit_percent is not None:
        user_income_percent_str = user_info["income_range"].replace("% ì´í•˜", "").strip()
        try:
            user_income_percent = int(user_income_percent_str)
            if user_income_percent > income_limit_percent:
                analysis_result["eligible"] = False
                analysis_result["unmet_conditions"].append(f"ì†Œë“ ê¸°ì¤€ ë¯¸ì¶©ì¡±: ìš”êµ¬ì‚¬í•­={income_limit_percent}% ì´í•˜, ì‚¬ìš©ì={user_income_percent}%")
            else:
                analysis_result["matched_conditions"].append(f"ì†Œë“ ê¸°ì¤€ ì¶©ì¡±: {income_limit_percent}% ì´í•˜ (ì‚¬ìš©ì {user_income_percent}%)")
        except ValueError:
             analysis_result["unmet_conditions"].append(f"ì†Œë“ ê¸°ì¤€ ê²€ì¦ ë¶ˆê°€: ì‚¬ìš©ì ì†Œë“ ì •ë³´ ì˜¤ë¥˜ ('{user_info['income_range']}')")


    # ì´ìì‚° ê²€ì¦
    total_asset_limit = extracted_criteria.get("total_asset_limit_won")
    if total_asset_limit is not None:
        user_total_assets = int(user_info["total_assets"])
        if user_total_assets > total_asset_limit:
            analysis_result["eligible"] = False
            analysis_result["unmet_conditions"].append(f"ì´ìì‚° ê¸°ì¤€ ë¯¸ì¶©ì¡±: ìš”êµ¬ì‚¬í•­={total_asset_limit:,.0f}ì› ì´í•˜, ì‚¬ìš©ì={user_total_assets:,.0f}ì›")
        else:
            analysis_result["matched_conditions"].append(f"ì´ìì‚° ê¸°ì¤€ ì¶©ì¡±: {total_asset_limit:,.0f}ì› ì´í•˜")

    # ìë™ì°¨ ìì‚° ê²€ì¦
    car_asset_limit = extracted_criteria.get("car_asset_limit_won")
    if car_asset_limit is not None:
        user_car_value = int(user_info["car_value"])
        # LLMì´ ì°¨ëŸ‰ ê°€ì•¡ ê¸°ì¤€ì„ ì–´ë–»ê²Œ í•´ì„í–ˆëŠ”ì§€ ì¤‘ìš” (0ì› ì´ˆê³¼ ë¶ˆê°€ vs íŠ¹ì • ê¸ˆì•¡ ì´í•˜)
        # ì˜ˆì‹œ ê³µê³ ë¬¸ì€ '3,803ë§Œì› ì´í•˜' ì´ë¯€ë¡œ ì´í•˜ë¡œ ë¹„êµ
        if user_car_value > car_asset_limit:
             analysis_result["eligible"] = False
             analysis_result["unmet_conditions"].append(f"ìë™ì°¨ ê¸°ì¤€ ë¯¸ì¶©ì¡±: ìš”êµ¬ì‚¬í•­={car_asset_limit:,.0f}ì› ì´í•˜, ì‚¬ìš©ì={user_car_value:,.0f}ì›")
        else:
             analysis_result["matched_conditions"].append(f"ìë™ì°¨ ê¸°ì¤€ ì¶©ì¡±: {car_asset_limit:,.0f}ì› ì´í•˜")
    # ë§Œì•½ LLMì´ ì°¨ëŸ‰ ë³´ìœ  ìì²´ë¥¼ ê¸ˆì§€í•˜ëŠ” ê²ƒìœ¼ë¡œ íŒë‹¨í–ˆë‹¤ë©´ (ì˜ˆ: car_allowed=False), ë¡œì§ ë³€ê²½ í•„ìš”

    # ë¬´ì£¼íƒ ìš”ê±´ ê²€ì¦
    must_be_homeless = extracted_criteria.get("must_be_homeless")
    if must_be_homeless is True:
         # user_infoì— ë¬´ì£¼íƒ ì—¬ë¶€ í•„ë“œê°€ í•„ìš”í•¨. ì—†ìœ¼ë¯€ë¡œ ì„ì‹œë¡œ 'ë¬´ì£¼íƒ'ìœ¼ë¡œ ê°€ì •.
         user_is_homeless = True # ì˜ˆì‹œ: ì‹¤ì œ user_infoì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨
         if not user_is_homeless:
              analysis_result["eligible"] = False
              analysis_result["unmet_conditions"].append("ë¬´ì£¼íƒ ì¡°ê±´ ë¯¸ì¶©ì¡±: ë¬´ì£¼íƒ ì„¸ëŒ€êµ¬ì„±ì›ì´ì–´ì•¼ í•¨")
         else:
              analysis_result["matched_conditions"].append("ë¬´ì£¼íƒ ì¡°ê±´ ì¶©ì¡±")
    # Falseë‚˜ nullì¸ ê²½ìš°ëŠ” ì¡°ê±´ì´ ì—†ê±°ë‚˜, ìœ ì£¼íƒìë„ ê°€ëŠ¥í•˜ê±°ë‚˜, ì¶”ì¶œ ì‹¤íŒ¨ë¡œ ê°„ì£¼

    # ë³µì¡í•œ ë…¼ë¦¬ ì¡°ê±´ (ì˜ˆ: AND, OR)ì€ í˜„ì¬ êµ¬ì¡°ì—ì„œ ì²˜ë¦¬í•˜ê¸° ì–´ë ¤ì›€
    # LLM í”„ë¡¬í”„íŠ¸ì—ì„œ ë” ë³µì¡í•œ êµ¬ì¡°ë¥¼ ìš”ì²­í•˜ê±°ë‚˜, í›„ì²˜ë¦¬ ë¡œì§ ê°•í™” í•„ìš”

    return analysis_result

def print_eligibility_result_llm(result):
    """ LLM ê¸°ë°˜ ìê²© ê²€ì¦ ê²°ê³¼ ì¶œë ¥ """
    print("\n" + "=" * 80)
    print(f"ğŸ“‹ ê³µê³ ë¬¸: {result['pdf_name']}")
    print("=" * 80)

    if result.get("eligible") is None:
        print(f"\nâš ï¸ ìê²© ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {result.get('reason', 'Unknown error')}")
    elif result["eligible"]:
        print("\nâœ… ì§€ì› ê°€ëŠ¥í•©ë‹ˆë‹¤!")
    else:
        print("\nâŒ ì§€ì› ìê²©ì´ ì¶©ì¡±ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    print("\n[LLM ì¶”ì¶œ ì¡°ê±´ (ì°¸ê³ )]")
    if result.get("extracted_criteria"):
        print(json.dumps(result["extracted_criteria"], indent=2, ensure_ascii=False))
    else:
        print("ì¶”ì¶œëœ ì¡°ê±´ ì •ë³´ ì—†ìŒ")


    print("\n[ì¶©ì¡±ëœ ì¡°ê±´]")
    if result.get("matched_conditions"):
        for condition in result["matched_conditions"]:
            print(f"âœ“ {condition}")
    else:
        print("ì¶©ì¡±ëœ ì¡°ê±´ ì—†ìŒ")

    if result.get("unmet_conditions"):
        print("\n[ë¯¸ì¶©ì¡±ëœ ì¡°ê±´]")
        for condition in result["unmet_conditions"]:
            print(f"âœ— {condition}")
    else:
         # ìê²© ë¯¸ë‹¬ì¸ë° ë¯¸ì¶©ì¡± ì¡°ê±´ì´ ì—†ëŠ” ê²½ìš° (ì˜¤ë¥˜ ê°€ëŠ¥ì„±)
         if result.get("eligible") == False:
              print("ë¯¸ì¶©ì¡±ëœ íŠ¹ì • ì¡°ê±´ì´ ì‹ë³„ë˜ì§€ ì•Šì•˜ìœ¼ë‚˜, ìê²© ìš”ê±´ ë¯¸ë‹¬ì…ë‹ˆë‹¤.")


    print("=" * 80)

# --- ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ ---
if __name__ == "__main__":
    # ì‚¬ìš©ì ì •ë³´ (ì˜ˆì‹œ)
    user_info = {
        "birth_date": "19990101", # ë§Œ ë‚˜ì´ ê³„ì‚° ìœ„í•´ YYYYMMDD í˜•ì‹ ê¶Œì¥
        "gender": "ë‚¨ì„±",
        "university_status": "ì¬í•™ ì¤‘", # LLM í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€í•˜ì—¬ ì¡°ê±´ ì¶”ì¶œ ìš”ì²­ ê°€ëŠ¥
        "recent_graduate": "ì˜ˆ",     # "
        "employed": "ì•„ë‹ˆì˜¤",        # "
        "job_seeking": "ì•„ë‹ˆì˜¤",       # "
        "household_type": "ìƒê³„Â·ì˜ë£ŒÂ·ì£¼ê±°ê¸‰ì—¬ ìˆ˜ê¸‰ì ê°€êµ¬", # "
        "parents_own_house": "ì•„ë‹ˆìš”", # "
        "disability_in_family": "ì˜ˆ", # "
        "application_count": 2,
        "total_assets": 100000000, # 1ì–µì› (í…ŒìŠ¤íŠ¸ ìš©ë„)
        "car_value": 5000000,      # 5ë°±ë§Œì› (í…ŒìŠ¤íŠ¸ ìš©ë„)
        "income_range": "100% ì´í•˜" # ì†Œë“ ë¶„ìœ„ ë˜ëŠ” ë¹„ìœ¨ í•„ìš”
        # ì¶”ê°€ í•„ë“œ: marital_status (Married/Unmarried), is_homeless (True/False) ë“± í•„ìš”
    }

    # ê²€ì¦í•  ê³µê³ ë¬¸ íŒŒì¼ëª…
    # pdf_name = "[ë§ˆì„ê³¼ì§‘]SHíŠ¹í™”í˜• ë§¤ì…ì„ëŒ€ì£¼íƒ(ì²­ë…„) ì…ì£¼ì ëª¨ì§‘ ê³µê³ ë¬¸_20250307.pdf"
    pdf_name = "[ë§ˆì„ê³¼ì§‘]SHíŠ¹í™”í˜• ë§¤ì…ì„ëŒ€ì£¼íƒ(ì²­ë…„) ì…ì£¼ì ëª¨ì§‘ ê³µê³ ë¬¸_20250307.pdf" # íŒŒì¼ í™•ì¥ì ì œê±° í•„ìš”

    # ìê²© ê²€ì¦ ì‹¤í–‰
    print(f"\nğŸš€ '{pdf_name}' ê³µê³ ë¬¸ì— ëŒ€í•œ ìê²© ê²€ì¦ ì‹œì‘ (LLM ê¸°ë°˜)")
    eligibility_result = check_eligibility_with_llm(pdf_name, user_info)

    # ê²°ê³¼ ì¶œë ¥
    print_eligibility_result_llm(eligibility_result)