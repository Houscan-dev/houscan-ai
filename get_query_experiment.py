import os
import torch
import json
import csv
import chromadb
from chromadb.config import Settings
from transformers import AutoTokenizer, AutoModel
import datetime
import re

# ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("BAAI/bge-m3")
model = AutoModel.from_pretrained("BAAI/bge-m3")
model.eval()

# ChromaDB í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
chroma_client = chromadb.PersistentClient(path="./chroma_db")
collection = chroma_client.get_or_create_collection(name="processed_chunks")

def get_embedding(text):
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
        embedding = outputs.last_hidden_state[:, 0, :]
    return embedding.squeeze().tolist()

def search_chunks(query_text, top_k=3, pdf_name=None):
    """ì²­í¬ ê²€ìƒ‰ í•¨ìˆ˜"""
    query_embedding = get_embedding("Represent this sentence for retrieval: " + query_text)
    
    # PDF íŒŒì¼ëª…ì´ ì§€ì •ëœ ê²½ìš° í•´ë‹¹ íŒŒì¼ ë‚´ì—ì„œë§Œ ê²€ìƒ‰
    if pdf_name:
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k,
            where={"filename": pdf_name}
        )
    else:
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k
        )

    print(f"\nğŸ” '{query_text}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ (Top {top_k}):")
    if results and results['ids'] and results['documents'] and results['metadatas']:
        for i in range(len(results['ids'][0])):
            metadata = results['metadatas'][0][i]
            content = results['documents'][0][i]
            
            print(f"\n[{i+1}] ë¬¸ì„œ: {metadata['filename']}")
            print(f"ì„¹ì…˜: {metadata.get('title', 'ì œëª© ì—†ìŒ')}")
            
            # ë‚´ìš© ì¶œë ¥ (ì œëª© íƒœê·¸ ì œê±°)
            content_without_title = re.sub(r'<h\d>.*?</h\d>\n?', '', content, 1)
            print(f"ë‚´ìš©: {content_without_title[:200]}...")  # ì²˜ìŒ 200ìë§Œ ì¶œë ¥
            print("-" * 80)
    else:
        print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    return results

def get_pdf_list():
    """ì €ì¥ëœ ëª¨ë“  PDF íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
    results = collection.get()
    if results and results['metadatas']:
        pdf_files = sorted(set(meta['filename'] for meta in results['metadatas']))
        print("\nğŸ“š ì €ì¥ëœ PDF íŒŒì¼ ëª©ë¡:")
        for i, pdf in enumerate(pdf_files, 1):
            print(f"{i}. {pdf}")
        return pdf_files
    return []

def get_pdf_sections(pdf_name):
    """íŠ¹ì • PDFì˜ ì„¹ì…˜ ëª©ë¡ ì¡°íšŒ"""
    results = collection.get(
        where={"filename": pdf_name}
    )
    
    if results and results['metadatas']:
        sections = sorted(set(meta.get('title', 'ì œëª© ì—†ìŒ') for meta in results['metadatas']))
        print(f"\nğŸ“‘ {pdf_name} ì„¹ì…˜ ëª©ë¡:")
        for i, section in enumerate(sections, 1):
            print(f"{i}. {section}")
        return sections
    return []

def get_pdf_content(pdf_name, section_title=None):
    """PDF ë‚´ìš© ì¡°íšŒ (ì „ì²´ ë˜ëŠ” íŠ¹ì • ì„¹ì…˜)"""
    where_clause = {"filename": pdf_name}
    if section_title:
        where_clause["title"] = section_title
    
    results = collection.get(where=where_clause)
    
    if not results or not results['metadatas']:
        print(f"âŒ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # ì²­í¬ ì¸ë±ìŠ¤ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    chunks = list(zip(results['metadatas'], results['documents']))
    chunks.sort(key=lambda x: x[0].get('chunk_index', 0))
    
    print(f"\nğŸ“„ {pdf_name}")
    if section_title:
        print(f"ì„¹ì…˜: {section_title}")
    print("=" * 80)
    
    for metadata, content in chunks:
        title = metadata.get('title', 'ì œëª© ì—†ìŒ')
        if not section_title:  # ì „ì²´ ë‚´ìš© ì¡°íšŒì‹œ ì„¹ì…˜ ì œëª© í‘œì‹œ
            print(f"\n## {title}")
        content_without_title = re.sub(r'<h\d>.*?</h\d>\n?', '', content, 1)
        print(content_without_title.strip())
        print("-" * 80)
    
    return chunks

def save_results_to_json(results, query_text, filename="search_results.json"):
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
    output = {
        "query": query_text,
        "results": []
    }

    if results and results['ids'] and results['documents'] and results['metadatas']:
        for i in range(len(results['ids'][0])):
            metadata = results['metadatas'][0][i]
            output["results"].append({
                "rank": i + 1,
                "id": results['ids'][0][i],
                "filename": metadata['filename'],
                "section": metadata.get('title', 'ì œëª© ì—†ìŒ'),
                "document": results['documents'][0][i]
            })

        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filepath = f"./results/{filename.replace('.json', '')}_{timestamp}.json"

        os.makedirs("results", exist_ok=True)
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(output, f, ensure_ascii=False, indent=2)

        print(f"âœ… ê²€ìƒ‰ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filepath}")
    else:
        print("âš ï¸ ì €ì¥í•  ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

def save_pdf_content(pdf_name, output_format="txt", section_title=None):
    """PDF ë‚´ìš©ì„ íŒŒì¼ë¡œ ì €ì¥"""
    content = get_pdf_content(pdf_name, section_title)
    if not content:
        return
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    section_suffix = f"_{section_title}" if section_title else ""
    filename = f"{pdf_name}_{timestamp}{section_suffix}.{output_format}"
    filepath = os.path.join("results", filename)
    
    os.makedirs("results", exist_ok=True)
    
    with open(filepath, "w", encoding="utf-8") as f:
        if output_format == "txt":
            for metadata, doc in content:
                title = metadata.get('title', 'ì œëª© ì—†ìŒ')
                f.write(f"\n## {title}\n")
                content_without_title = re.sub(r'<h\d>.*?</h\d>\n?', '', doc, 1)
                f.write(content_without_title.strip() + "\n\n")
                f.write("-" * 80 + "\n")
        elif output_format == "json":
            json.dump({
                "filename": pdf_name,
                "section": section_title,
                "content": [{
                    "title": meta.get('title', 'ì œëª© ì—†ìŒ'),
                    "text": re.sub(r'<h\d>.*?</h\d>\n?', '', doc, 1).strip()
                } for meta, doc in content]
            }, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ë‚´ìš©ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filepath}")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # 1. PDF ëª©ë¡ í™•ì¸
    print("\n=== ì €ì¥ëœ PDF íŒŒì¼ ëª©ë¡ ===")
    pdf_files = get_pdf_list()
    if not pdf_files:
        print("ì €ì¥ëœ PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        exit()

    # 2. íŠ¹ì • PDF ì„ íƒ (ì˜ˆì‹œ)
    selected_pdf = pdf_files[0]
    
    # 3. ì„ íƒëœ PDFì˜ ì„¹ì…˜ ëª©ë¡ í™•ì¸
    print(f"\n=== {selected_pdf} ì„¹ì…˜ ëª©ë¡ ===")
    sections = get_pdf_sections(selected_pdf)
    
    # 4. ê²€ìƒ‰ ì˜ˆì‹œ
    query = "ì²­ì•½ ì‹ ì²­ ìê²© ì•Œë ¤ì¤˜"
    
    # 4.1 ì „ì²´ PDFì—ì„œ ê²€ìƒ‰
    print("\n=== ì „ì²´ PDF ê²€ìƒ‰ ===")
    all_results = search_chunks(query, top_k=3)
    
    # 4.2 íŠ¹ì • PDFì—ì„œë§Œ ê²€ìƒ‰
    print(f"\n=== {selected_pdf} ê²€ìƒ‰ ===")
    pdf_results = search_chunks(query, top_k=3, pdf_name=selected_pdf)
    
    # 5. íŠ¹ì • PDFì˜ ì „ì²´ ë‚´ìš© ì €ì¥
    save_pdf_content(selected_pdf, output_format="txt")
    
    # 6. íŠ¹ì • PDFì˜ íŠ¹ì • ì„¹ì…˜ ë‚´ìš© ì €ì¥ (ì„¹ì…˜ì´ ìˆëŠ” ê²½ìš°)
    if sections:
        save_pdf_content(selected_pdf, output_format="txt", section_title=sections[0])
    
    # 7. ê²€ìƒ‰ ê²°ê³¼ ì €ì¥
    save_results_to_json(all_results, query)