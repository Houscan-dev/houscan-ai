import os
import torch
import json
import chromadb
from chromadb.config import Settings
from transformers import AutoTokenizer, AutoModel

# ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("BAAI/bge-m3")
model = AutoModel.from_pretrained("BAAI/bge-m3")
model.eval()

# ìƒˆ í´ë¼ì´ì–¸íŠ¸ ìƒì„± ë°©ì‹
chroma_client = chromadb.PersistentClient(path="./chroma_db")

collection = chroma_client.get_or_create_collection(name="my_chunks")


# í•¨ìˆ˜: CLS í† í° ë²¡í„° ì¶”ì¶œ
def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
        embedding = outputs.last_hidden_state[:, 0, :]
    return embedding.squeeze().tolist()

def search_chunks(query_text, top_k=3):
    # ì¿¼ë¦¬ ë¬¸ì¥ ì„ë² ë”© (BGE ìŠ¤íƒ€ì¼: Instruction ì¶”ê°€)
    query_embedding = get_embedding("Represent this sentence for retrieval: " + query_text)
    
    # ChromaDBì—ì„œ ìœ ì‚¬í•œ ì²­í¬ ê²€ìƒ‰
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=top_k
    )

    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ” '{query_text}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ (Top {top_k}):\n")
    for i in range(len(results['ids'][0])):
        print(f"[{i+1}] ë¬¸ì„œ ID: {results['ids'][0][i]}")
        print(f"íŒŒì¼ëª…: {results['metadatas'][0][i]['filename']}")
        print(f"ë‚´ìš©: {results['documents'][0][i][:150]}...")  # ì²˜ìŒ 150ìë§Œ ì¶œë ¥
        print("-" * 60)


search_chunks("ê°•ë‚¨êµ¬ ë¬´ìˆœìœ„ ì²­ì•½ ì¡°ê±´ ì•Œë ¤ì¤˜", top_k=5)